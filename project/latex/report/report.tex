\documentclass[conference, twoside]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{listings}
\usepackage{lipsum}
\usepackage{import}
% listing settings
\lstset{
  frame = lines,
  language = C,
  basicstyle = \ttfamily\footnotesize,
  breaklines = true,
}
% listing caption redefinition
\makeatletter
\def\lst@makecaption{%
  \def\@captype{table}%
  \@makecaption
}
\makeatother

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\bstctlcite{IEEEexample:BSTcontrol} % remove dashed repeated authors

\title{Investigating Performance Improvements\\in Discrete Space Hartree-Fock Calculations\\Through Graphics Processing Units\\{\large EECE542 Final Project Report}}
\author{\IEEEauthorblockN{Michel Kakulphimp}
\IEEEauthorblockA{\textit{Dept. of Electrical and Computer Engineering}\\
\textit{University of British Columbia}\\
Vancouver, Canada\\
michel@kakulphimp.net}
}

\markboth{UBC Winter 2021 EECE542: Nanoscale Modelling and Simulations}%
{}
\maketitle

\section{Introduction}

\section{Background and Related Work}

\subsection{Hartree-Fock Method}

The Hartree-Fock (HF) method \cite{szabo-ostlund} is designed to help approximate a solution to the Schr\"{o}dinger equation for many-electron systems. This is accomplished by making some key approximations, including: the Born-Oppenheimer approximation to fix the kinematics of the nucleus, using a single Slater determinant composed of orthogonal molecular spin orbitals to satisfy the Pauli exclusion principle, and most importantly approximating the Hamiltonian as a single electron function by treating the other electrons in the system as a mean field rather than evaluating with their exact states. HF iteratively converges to its solution by progressively making better guesses at the structure of the mean field of the system. Combined, these techniques help reduce the original multivariate and computationally intensive problem into one that is easily solved by a computer program. One such implementation is described within this report, where we perform the restricted, closed-shell HF algorithm on a Helium atom.

Since Helium has an even number of electrons that close shells, we can use the restricted HF (RHF) equation for closed shell systems to numerically calculate the resulting orbitals. The equation is as follows:

\begin{align}
  \hat{F}(\vec{r})\psi_n(\vec{r}) &= \epsilon_n\psi_n(\vec{r})
\end{align}

Where $\hat{F}(\vec{r})$ is the Fock operator which is defined as follows:

\begin{align}
  \hat{F}(\vec{r}) &= \hat{H}_{core}(\vec{r}) + \sum_{n=1}^{N/2}\left[2J_n(\vec{r}) - K_n(\vec{r})\right]
\end{align}

The Fock operator is composed of the core Hamiltonian operator $\hat{H}(\vec{r})$, the Coulomb operator $\hat{J}(\vec{r})$, and the exchange operator $\hat{K}(\vec{r})$.

\begin{align}
  \hat{H}_{core}(\vec{r}) &= -\frac{1}{2}\nabla_1^2 - \sum_A\frac{Z_A}{\|\vec{r}_{1 A}\|}\\
  \hat{J}_j(\vec{r}_1)\psi_i(\vec{r}_1) &= \psi_i(\vec{r}_1)\int_{-\infty}^{\infty}\left|\psi_i(\vec{r}_2)\right|^2\frac{1}{\|\vec{r}_{12}\|}d\vec{r}_2 \\
  \hat{K}_j(\vec{r}_1)\psi_i(\vec{r}_1) &= \psi_j(\vec{r}_1)\int_{-\infty}^{\infty}\frac{\psi_j^\ast(\vec{r}_2)\psi_i(\vec{r}_2)}{\|\vec{r}_{12}\|}d\vec{r}_2
\end{align}

By discretizing the solution space of the problem into $N$ partitions in the three Cartesian coordinate directions and representing the Fock equation in matrix form, we can solve for the discretized wave equation $\psi_n(\vec{r})$ by solving for the eigenvectors of the Fock matrix. For each iteration of the HF algorithm, a new guess is obtained for the discretized wave equation, which can be used to compute the Fock matrix for the following iteration. If initial conditions are favorable, the iterations will eventually settle on a solution with little to no variation. The numeric value of the total energy of the system on every iteration can be used as an indicator for convergence.

This report does not take the concept of HF further than what is presented; however, it is important to note that there is are numerous improvement on the technique. One such improvement uses the Roothaan-Hall equations, by representing the solution wave functions through a linear combination of known spatial basis equations. This allows HF to take another matrix form and has the benefit of being much lighter in terms of computation. Since the techniques discussed in the report target fundamental computations shared by HF and its derivatives, they can of course be applied to them as well.

\subsection{CUDA Parallel Computing Platform}

Graphics processing units (GPUs) are primarily designed to accelerate and support the computational requirements of real-time raster 3D graphics. However, their hardware is also well suited for accelerating other, highly parallelizable workloads. This is made possible through GPU manufacturers providing APIs that allow developers to write specially designed software to run on the GPU processing units. Since GPUs have a fundamentally different architecture than CPUs, the software must be written with the architecture in mind and must also be compiled as a separate executable entity to be run exclusively on the GPU.

In GPU terminology, the host is the CPU that the GPU is coupled to and the device is the GPU itself. The host is what will be running the main program and the device will be executing offloaded accelerated portions of the main program. The device is connected to the host via a peripheral component interconnect, which by modern standards will likely be PCIe. The functions running on the device are called kernels. When a developer is designing a kernel, the API provides mechanisms to define how that kernel will be parallelized on the device. This would allow, for example, the developer to slice up a problem among the GPU's processing units in a programmatic way.

In an example implementation of GPU acceleration, a developer identifies workloads that are well suited for the architecture of a GPU to accelerate and write routines to be executed exclusively on the GPU. The host program would then call these routines and provide the necessary data for the device to perform execution and return results.

NVIDIA, a GPU manufacturer, provides one such API for developers called Computer Unified Architecture or CUDA. It is also known as the CUDA parallel computing platform \cite{nvidia-cuda}. CUDA is only available for NVIDIA GPU hardware, but there also exists an alternative that works on other GPU vendors called OpenCL. Since an NVIDIA-based GPU was available for use in this project, CUDA was chosen as the API for GPU acceleration.

\cite{electronic-structure-calculations-on-gpus}

\section{Acceleration Opportunities}

\subsection{Numerical Integration}

\subsection{Eigensolver}

\section{Implementation}

For this investigation, a program written in C++ was created to evaluate the acceleration of the eigensolver and the numerical integration on a discrete space HF solver for the Helium atom. The program is designed to be able to variably define the number of partitions used in the discretization of the solution space and selectively run the eigensolver and numerical integration on either the CPU or the GPU. C++ was chosen as the programming language as it is straightforward to integrate with the CUDA C APIs and the language has wide support in terms of libraries for utilities. To facilitate the linear algebra operations required by HF, the Eigen \cite{eigen} C++ template library was chosen. This provides structures and types to support matrix arithmetic. Eigen was linked with AMD Basic Linear Algebra Subprograms (BLAS) libraries \cite{amd-blas} to provide low level multi-core CPU support for matrix operations. Eigen was also linked with LAPACK (Linear Algebra PACKage) \cite{lapack} libraries which are used in higher-level linear algebra operations such as those to compute the eigenvectors and eigenvalues of a matrix. OpenMP is used to enable multiprocessor support for the CPU BLAS and LAPACK operations, and the LAPACKE C API is used to wrap around the LAPACK libraries which are written in FORTRAN. Finally, the C++ Boost libraries provide the program with quality of life tools for things such as command line arguments parsing and string formatting.

Due to the locality requirements of the data to be computed on, the program is architected in such a way where GPU accelerated routine data can either be allocated in RAM or in VRAM. In a CPU-only run of a routine, the data is only allocated on the CPU side and remains resident in RAM. However, in the GPU-accelerated run of a routine, data required for computation is allocated and copied over to the GPU's VRAM over PCIe. Once GPU computation is complete, the resulting data is copied back to the host RAM. The Eigen template library provides a means to map its matrix and vector structures over a pointer to data in memory, allowing for this dynamically allocated data to be computed on during matrix arithmetic.

The number of partitions in the solution space is passed as an argument into the simulator. This defines how the three dimensions of the space are divided in all three Cartesian coordinate directions. To calculate the solution, every possible coordinate in the three dimensional space is placed along the dimensions of a solution matrix. If $N$ partitions are chosen, then the resulting matrix has dimensions of $N^3 \times N^3$. Increasing the partition size of the problem imposes an exponential growth of the solution space, which is evident in the runtime of the program. Arranging the problem in this matrix form allows for the implementation of the second order differential operator ($\nabla^2$) that exists in the kinetic energy term which relies on neighboring values. The exchange, Coulombic attraction/repulsion, are sparse diagonal matrices and are stored as such in the program to save on memory. When accessing elements of the matrices, the program will often only iterate over a single dimension of matrix. This is largely due to many of the matrices only requiring their diagonals to be evaluated. This has the benefit of simplifying the code and also maximizing the effectiveness of the GPU to work on its data, as it works best on sequential, ordered data \cite{special-purpose-hf-computer}.

Program configuration values are stored in structs which are passed around the program's execution, including to functions that are executed on the GPU. Some data in these structures are large enough to warrant semi-permanent residence in the GPU's memory, so these are copied over to the GPU in bulk so that they remain available for GPU calculations. These values also include those that would have incurred repeated calculations of the same data. Having them calculated once at startup and stored in these structs allows for some optimizations in the program's execution on both the CPU and GPU.

\section{Results}

The results for this investigation were obtained using a personal computing platform with the following relevant technical specifications:

\begin{itemize}
    \item CPU: AMD Ryzen 9 5950X 16-Core Processor
    \item RAM: 64 GB DDR4
    \item GPU: NVIDIA GeForce RTX 3080 Ti
    \item VRAM: 12 GB GDDR6X 
    \item CPU-GPU Interconnect: PCIe 4.0 x16 (32 GB/s maximum bandwidth)
\end{itemize}

\begin{figure*}[ht]
\centering
\includegraphics[width=7in]{figures/sixteen-core-results.pdf}
\caption{Performance Results Per Iteration (16 CPU Threads)}
\label{perf-results-per-iteration-sixteen-core}
\end{figure*}

% Mention overhead that show up in results when integrating on GPU

\begin{table*}
    \renewcommand{\arraystretch}{1.3} % vertically stretch table out
    \caption{CPU Only Results}
    \label{cpu-only-results}
    \centering
    \begin{tabular}{c||c|c|c|c|c}
        \hline
        {Solution Partitions} & {10} & {15} & {20} & {25} & {30} \\
        \hline
        \hline
        {Eigensolver Time (s)}              & {0.0884} & {1.0962} & {16.853} & {160.63} & {877.85}\\
        {Integration Time (s)}              & {0.0228} & {0.1649} & {0.8228} & {3.1488} & {9.3617}\\
        {Total Time (s)}                    & {0.1141} & {1.3169} & {17.977} & {164.99} & {891.47}\\
        \hline
    \end{tabular}
\end{table*}

\begin{table*}
    \renewcommand{\arraystretch}{1.3} % vertically stretch table out
    \caption{GPU Eigensolver Results}
    \label{gpu-eigensolver-results}
    \centering
    \begin{tabular}{c||c|c|c|c|c}
        \hline
        {Solution Partitions} & {10} & {15} & {20} & {25} & {30} \\
        \hline
        \hline
        {Eigensolver Time (s)}              & {0.7677} & {0.9546} & {1.972}  & {7.3264} & {28.903}\\
        {Integration Time (s)}              & {0.0125} & {0.1433} & {0.8101} & {3.123}  & {9.1942}\\
        {Total Time (s)}                    & {0.7835} & {1.1514} & {3.0773} & {11.789} & {41.99} \\
        \hline
    \end{tabular}
\end{table*}

\begin{table*}
    \renewcommand{\arraystretch}{1.3} % vertically stretch table out
    \caption{GPU Integration Results}
    \label{gpu-integration-results}
    \centering
    \begin{tabular}{c||c|c|c|c|c}
        \hline
        {Solution Partitions} & {10} & {15} & {20} & {25} & {30} \\
        \hline
        \hline
        {Eigensolver Time (s)}              & {0.0905} & {1.0869} & {16.844} & {159.43} & {880.25}\\
        {Integration Time (s)}              & {0.1473} & {0.1396} & {0.1488} & {0.1514} & {0.1491}\\
        {Total Time (s)}                    & {0.2411} & {1.2816} & {17.295} & {160.71} & {883.72}\\
        \hline
    \end{tabular}
\end{table*}

\begin{table*}
    \renewcommand{\arraystretch}{1.3} % vertically stretch table out
    \caption{GPU Eigensolver \& GPU Integration Results}
    \label{gpu-eigensolver-and-integration-results}
    \centering
    \begin{tabular}{c||c|c|c|c|c}
        \hline
        {Solution Partitions} & {10} & {15} & {20} & {25} & {30} \\
        \hline
        \hline
        {Eigensolver Time (s)}              & {0.6014} & {0.8232} & {1.8666} & {6.95}   & {28.565}\\
        {Integration Time (s)}              & {0.2445} & {0.2478} & {0.2618} & {0.2837} & {0.3606}\\
        {Total Time (s)}                    & {0.8501} & {1.1239} & {2.4401} & {8.422}  & {32.54} \\
        \hline
    \end{tabular}
\end{table*}

\section{Future Work}

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, refs}

\end{document}